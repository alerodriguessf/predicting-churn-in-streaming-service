{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GknVE-iIS508_Jr_cYJ2X6YTw6P4L-zD",
      "authorship_tag": "ABX9TyNrRt9YWbqxnm150f+4yiJ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alerodriguessf/predicting-churn-in-streaming-service/blob/main/Portfolio_Predicting_Churn_in_Streaming_Service_Logistic_Regression_20250117.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predicting Churn in Streaming Service using Logistic Regression\n"
      ],
      "metadata": {
        "id": "gDq9oBCFEJUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import fundamental libraries for data analysis\n",
        "!pip install ydata-profiling\n",
        "!pip install --upgrade numba\n",
        "!pip install sidetable\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ydata_profiling import ProfileReport\n",
        "import sidetable\n",
        "import missingno as msno\n",
        "from ipywidgets import interact, widgets\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import scale, minmax_scale, power_transform"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hHBFhBGRGapX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT9Oq4UTFw_v"
      },
      "outputs": [],
      "source": [
        "# Upload the challenge file\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 01: Exploratory Data Analysis (Data Understanding)\n",
        "1. Load the dataset;\n",
        "2. Perform a statistical description of the data;\n",
        "3. Check the data types;\n",
        "4. Check the amount of missing values.\n"
      ],
      "metadata": {
        "id": "9fK0WRFKGtsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the main dataset\n",
        "\n",
        "df = pd.read_excel('streaming_data (1).xlsx')"
      ],
      "metadata": {
        "id": "mtdETZmhGj69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.profile_report()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jvCqvxlUGsod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TYNOyXFDHhGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SQjnvZ3Legep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 02: Data Preparation\n",
        "1. Replace \"NaN\" values with 0 in specific columns;\n",
        "2. Remove rows with null values in essential columns;\n",
        "3. Map \"churned\" values to \"No\" and \"Yes\";\n",
        "4. Convert float columns to integers."
      ],
      "metadata": {
        "id": "gVXLjCf9HpFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values with 0 in specific columns\n",
        "\n",
        "columns_replace= ['Time_on_platform', 'Num_streaming_services', 'Churned', 'Avg_rating', 'Devices_connected']\n",
        "df[columns_replace] = df[columns_replace].fillna(0)"
      ],
      "metadata": {
        "id": "mko1euwUHqQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with missing values in essential columns\n",
        "\n",
        "df.dropna(subset=['Gender', 'Subscription_type', 'Age'], inplace=True)"
      ],
      "metadata": {
        "id": "BUnBZUCRHvMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform \"Churned\" values from 0/1 to \"No\"/\"Yes\"\n",
        "\n",
        "df['Churned'] = df['Churned'].replace({0: 'No', 1: 'Yes'})\n",
        "\n",
        "\n",
        "df.head(10)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "d_3VmbAHHxmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert float columns to integers\n",
        "\n",
        "df['Age'] = df['Age'].astype(int)\n",
        "df['Time_on_platform'] = df['Time_on_platform'].astype(int)\n",
        "df['Num_streaming_services'] = df['Num_streaming_services'].astype(int)\n",
        "df['Avg_rating'] = df['Avg_rating'].astype(int)\n",
        "df['Devices_connected'] = df['Devices_connected'].astype(int)\n",
        "\n",
        "# Check dataset information after data preparation\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "daStENPGH3n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore possible outliers in the data using boxplot\n",
        "\n",
        "from sklearn.preprocessing import scale\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "\n",
        "\n",
        "df[numeric_cols].apply(scale).plot.box()\n",
        "\n",
        "plt.xticks(rotation=90, ha='right');"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w-hr9eC6H_35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse correlations between numerical variables\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "numerical_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "correlation_matrix = numerical_df.corr()\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rkSq8YLzIE6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 03: Data Modelling - Logistic Regression\n",
        "1. Define X (features) and y (target) variables;\n",
        "2. Scale the data using MinMaxScaler;\n",
        "3. Split the dataset into training and testing sets;\n",
        "4. Fit the Logistic Regression model;\n",
        "5. Evaluate the model using a confusion matrix and classification metrics."
      ],
      "metadata": {
        "id": "Vwt0LApHINcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
      ],
      "metadata": {
        "id": "mX5OG8dbIMXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define independent (X) and dependent (y) variables\n",
        "\n",
        "x = df[['Age','Devices_connected','Time_on_platform','Num_streaming_services','Avg_rating']]\n",
        "y = df['Churned'].replace({'No': 0, 'Yes': 1})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9aq5wz4IJCnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "scaler = MinMaxScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "RSAH15IrP57w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Logistic Regression model\n",
        "\n",
        "log_model = LogisticRegression(random_state = 42)\n",
        "log_model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "TeI06PUdQErA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "\n",
        "y_pred = log_model.predict(x_test)\n",
        "\n",
        "# Display confusion matrix and classification report\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=log_model.classes_)\n",
        "disp.plot(cmap = 'viridis')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "HlcJluh9QLyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 04: Model Optimisation with Grid Search\n",
        "1. Use GridSearchCV to find the best parameters;\n",
        "2. Train the optimised model;\n",
        "3. Evaluate the performance of the optimized model."
      ],
      "metadata": {
        "id": "1fCLzMc2Qs5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid for Logistic Regression\n",
        "\n",
        "param_grid = {\n",
        " 'C':[0.1,1,10,100],\n",
        " 'solver' : ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Best model found by Grid Search\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "print(f'best parameters: {grid_search.best_params_}')\n",
        "\n"
      ],
      "metadata": {
        "id": "P3aN64VIQt4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate optimised model\n",
        "\n",
        "y_pred = best_model.predict(x_test)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['No','Yes'])\n",
        "disp.plot(cmap = 'viridis')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "PQ07REg4RtRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 05: Modelling with Random Forest\n",
        "1. Train a Random Forest model;\n",
        "2. Optimise hyperparameters using Grid Search;\n",
        "3. Evaluate the model with a confusion matrix and metrics.\n"
      ],
      "metadata": {
        "id": "v5wcKpWfR7CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions with Random Forest\n",
        "y_pred_rf = rf_model.predict(x_test)\n",
        "\n",
        "# Display confusion matrix for Random Forest\n",
        "ConfusionMatrixDisplay.from_estimator(rf_model,x_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "# Display classification report for Random Forest\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "8MnJc7CjOhGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimise hyperparameters for Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 10]\n",
        "}\n",
        "\n",
        "rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5)\n",
        "rf_grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Best Random Forest model found by Grid Search\n",
        "\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "\n",
        "y_pred_best_rf = best_rf_model.predict(x_test)\n",
        "\n",
        "# Evaluate optimized model\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(best_rf_model, x_test, y=y_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4tWwJhIZOyc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best parameters:\", rf_grid_search.best_params_)\n",
        "print(classification_report(y_test, y_pred_best_rf))\n"
      ],
      "metadata": {
        "id": "_6lhkRM0O38g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YtSkVWe_NvhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for the user input\n",
        "user_data = pd.DataFrame({\n",
        "    'Age': [12],\n",
        "    'Devices_connected': [4],\n",
        "    'Time_on_platform': [1],\n",
        "    'Num_streaming_services': [2],\n",
        "    'Avg_rating': [1]\n",
        "})\n",
        "\n",
        "# Scale the user input data using the trained scaler\n",
        "user_data_scaled = scaler.transform(user_data)\n",
        "\n",
        "# Predict the probability of churn\n",
        "churn_probability = best_model.predict_proba(user_data_scaled)[0][1]\n",
        "\n",
        "# Display the result\n",
        "print(f\"The predicted probability of churn for this user is: {churn_probability:.3%}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kbJtmA6rM6R7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}